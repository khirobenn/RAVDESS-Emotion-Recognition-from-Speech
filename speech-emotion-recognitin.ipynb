{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob.glob(\"./archive/*/*\")\n",
    "print(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43f699",
   "metadata": {},
   "source": [
    "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
    "\n",
    "Filename identifiers\n",
    "\n",
    "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "\n",
    "Vocal channel (01 = speech, 02 = song).\n",
    "\n",
    "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "\n",
    "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "\n",
    "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "\n",
    "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "\n",
    "Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "Filename example: 03-01-06-01-02-01-12.wav\n",
    "\n",
    "Audio-only (03)\n",
    "Speech (01)\n",
    "Fearful (06)\n",
    "Normal intensity (01)\n",
    "Statement \"dogs\" (02)\n",
    "1st Repetition (01)\n",
    "12th Actor (12)\n",
    "Female, as the actor ID number is even.\n",
    "\n",
    "So we have to make the outputs for each audio, it means we have to select only the emotion for each audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"]\n",
    "Y = []\n",
    "# We will iterate in a copy of audio_files to delete all the non-audio files\n",
    "for audio in audio_files[:]:\n",
    "    if not audio.endswith(\"wav\"):\n",
    "        audio_files.remove(audio)\n",
    "    else:\n",
    "        Y.append(int(audio.split(\"-\")[-5]) -1)\n",
    "\n",
    "Y = np.array(Y)\n",
    "Y = Y.astype(np.int32)\n",
    "\n",
    "print(len(audio_files))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59606d7",
   "metadata": {},
   "source": [
    "# Preparing my inputs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(audio_path, sr=22050):\n",
    "    audio_file, m = librosa.load(path=audio_path, sr=sr, duration=3.5)\n",
    "\n",
    "    # We make sure our sound is 3.5 seconds before applying all the other functions on it\n",
    "    if(len(audio_file) < int(sr*3.5)):\n",
    "        audio_file = librosa.util.fix_length(audio_file, size=int(3.5*sr))\n",
    "    \n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio_file, n_mels=256, sr=sr)\n",
    "    mel_spectogram_db = librosa.power_to_db(mel_spec)\n",
    "    # normalize the sample\n",
    "    mel_spectogram_db = mel_spectogram_db.astype(np.float32)\n",
    "    \n",
    "    return mel_spectogram_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b98a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_set(audio_files):\n",
    "    X = []\n",
    "\n",
    "    for audio in audio_files:\n",
    "        X.append(prepare_data(audio))\n",
    "\n",
    "    return np.stack(X)\n",
    "\n",
    "X = prepare_data_set(audio_files)\n",
    "X = (X - np.min(X)) / (np.max(X) - np.min(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f8fbc",
   "metadata": {},
   "source": [
    "# Viewing some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c474360",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 90\n",
    "\n",
    "audio_file, m = librosa.load(audio_files[index], sr=22050)\n",
    "print(len(audio_file))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.plot(audio_file)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fc7f9",
   "metadata": {},
   "source": [
    "# Viewing spectrograms and melspectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ffbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # We apply short time fourier transform\n",
    "audio_file_transformed = librosa.stft(audio_file)\n",
    "# We make the spectogram\n",
    "spectogram = np.abs(audio_file_transformed)\n",
    "spectogram_db = librosa.amplitude_to_db(spectogram)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sp = librosa.display.specshow(spectogram_db, x_axis=\"time\", y_axis=\"log\", ax=ax)\n",
    "ax.set_title(emotions[Y[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fe23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using mel spectrogram:\n",
    "my_example = prepare_data(audio_files[index])\n",
    "print(my_example)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sp = librosa.display.specshow(my_example, x_axis=\"time\", y_axis=\"log\", ax=ax)\n",
    "ax.set_title(emotions[Y[index]] + \" mel_spec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835115e6",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(X.shape[1:] + (1,)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32 , (3,3), activation=\"relu\"))\n",
    "model.add(keras.layers.AveragePooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64 , (3,3), activation=\"relu\"))\n",
    "model.add(keras.layers.AveragePooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation=\"relu\"))\n",
    "model.add(keras.layers.AveragePooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(256, (3,3), activation=\"relu\"))\n",
    "model.add(keras.layers.AveragePooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512,  activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2897bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*len(X))\n",
    "\n",
    "X_train = X[:train_size]\n",
    "Y_train = Y[:train_size]\n",
    "\n",
    "X_test = X[train_size:]\n",
    "Y_test = Y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"model.keras\", monitor=\"val_accuracy\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=64, epochs=100, callbacks=checkpoint, validation_split=0.2)\n",
    "\n",
    "test_scores = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0073317",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = model.evaluate(X_train, Y_train, verbose=2)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(test_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
